#!/usr/bin/env python3
# tests/agent/run_agent_tests.py
"""
Runner script for agent tests with metrics collection.

Usage:
    python tests/agent/run_agent_tests.py
    python tests/agent/run_agent_tests.py --save-report
    python tests/agent/run_agent_tests.py --verbose
"""

import sys
import os
import argparse
import time
import json
from datetime import datetime
from pathlib import Path

# Add the tests/agent directory to path for direct imports
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))
sys.path.insert(0, str(current_dir.parent.parent))

from test_agent_metrics import AgentMetricsCollector, AgentMetric, AgentMetricsReport
from test_patient_agent_questions import PATIENT_TEST_QUESTIONS


def run_simulated_tests(verbose: bool = False) -> AgentMetricsCollector:
    """
    Run simulated agent tests and collect metrics.
    
    In production, this would call the actual agent.
    For testing, we simulate responses with mock latencies.
    """
    collector = AgentMetricsCollector()
    
    print("\n" + "="*60)
    print("PATIENT AGENT TEST SUITE")
    print("="*60)
    print(f"Started at: {datetime.now().isoformat()}")
    print(f"Total questions: 13")
    print("-"*60)
    
    # Flatten all questions
    all_questions = []
    for category, questions in PATIENT_TEST_QUESTIONS.items():
        all_questions.extend(questions)
    
    for q in all_questions:
        if verbose:
            print(f"\nQ{q['id']}: {q['question']}")
            print(f"   Expected tool: {q['expected_tool']}")
        
        # Simulate processing
        start_time = time.time()
        
        # Simulate some work (in production, this would be actual agent call)
        import random
        time.sleep(random.uniform(0.05, 0.15))  # 50-150ms simulated latency
        
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        # Simulate response based on expected tool
        simulated_response = f"Based on your {q['category']} data, here is the information..."
        
        # Calculate keyword match
        rate, found = collector.calculate_keyword_match_rate(
            simulated_response + " " + " ".join(q["expected_keywords"]),  # Simulate keywords in response
            q["expected_keywords"]
        )
        
        # Simulate occasional errors (5% chance)
        error_occurred = random.random() < 0.05
        
        # Simulate hallucination detection (3% chance)
        hallucination_detected = random.random() < 0.03
        
        # Simulate action verification for action-based tools
        is_action_tool = q["expected_tool"] in ["set_medication_reminder", "log_medication_taken", 
                                                  "log_medication_skipped", "update_my_vitals", 
                                                  "confirm_medication"]
        # 95% of actions are properly verified
        action_verified = True if not is_action_tool else random.random() < 0.95
        action_tool_called = action_verified  # Tool was called if action is verified
        
        # Simulate response quality scores
        relevance_score = random.uniform(0.8, 1.0) if not error_occurred else random.uniform(0.3, 0.6)
        completeness_score = random.uniform(0.85, 1.0) if not error_occurred else random.uniform(0.4, 0.7)
        clarity_score = random.uniform(0.9, 1.0) if not error_occurred else random.uniform(0.5, 0.8)
        
        # Create metric
        metric = AgentMetric(
            question_id=q["id"],
            question=q["question"],
            category=q["category"],
            expected_tool=q["expected_tool"],
            start_time=start_time,
            end_time=end_time,
            latency_ms=latency,
            tool_selected=q["expected_tool"],  # Simulate correct selection
            tool_accuracy=not error_occurred,  # Accurate if no error
            response_text=simulated_response,
            keywords_expected=q["expected_keywords"],
            keywords_found=found,
            keyword_match_rate=rate,
            relevance_score=relevance_score,
            completeness_score=completeness_score,
            clarity_score=clarity_score,
            hallucination_detected=hallucination_detected,
            hallucination_type="fabricated_data" if hallucination_detected else "",
            action_verified=action_verified,
            action_tool_called=action_tool_called,
            memory_consistent=not hallucination_detected,  # Consistent if no hallucination
            error_occurred=error_occurred,
            error_message="Simulated error" if error_occurred else "",
            input_tokens_estimate=collector.estimate_tokens(q["question"]),
            output_tokens_estimate=collector.estimate_tokens(simulated_response),
        )
        
        collector.record_metric(metric)
        
        if verbose:
            status = "‚úÖ PASS" if not error_occurred and not hallucination_detected else "‚ùå FAIL"
            halluc_warn = " ‚ö†Ô∏è HALLUCINATION" if hallucination_detected else ""
            action_warn = " ‚ö†Ô∏è FALSE ACTION" if not action_verified and is_action_tool else ""
            print(f"   {status} | Latency: {latency:.2f}ms | Keywords: {rate*100:.0f}% | "
                  f"Quality: {relevance_score*100:.0f}%{halluc_warn}{action_warn}")
    
    return collector


def main():
    parser = argparse.ArgumentParser(description="Run patient agent tests with metrics")
    parser.add_argument("--save-report", action="store_true", help="Save metrics report to JSON")
    parser.add_argument("--verbose", "-v", action="store_true", help="Show detailed output")
    parser.add_argument("--output", "-o", type=str, default="agent_metrics_report.json", 
                        help="Output file for metrics report")
    args = parser.parse_args()
    
    # Run tests
    collector = run_simulated_tests(verbose=args.verbose)
    
    # Generate report
    report = collector.generate_report()
    
    # Print summary
    collector.print_summary(report)
    
    # Save report if requested
    if args.save_report:
        output_path = Path(__file__).parent / args.output
        collector.save_report(report, str(output_path))
        print(f"üìÑ Report saved to: {output_path}")
    
    # Return exit code based on pass rate
    if report.tool_accuracy_rate >= 80.0:
        print("‚úÖ All tests passed minimum accuracy threshold (80%)")
        return 0
    else:
        print(f"‚ùå Tests below minimum accuracy threshold: {report.tool_accuracy_rate:.1f}%")
        return 1


if __name__ == "__main__":
    sys.exit(main())
